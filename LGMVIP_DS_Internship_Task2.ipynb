{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the stock price data\n",
        "data = pd.read_excel('/content/LGMVIP_DS_Internship-Task2.xlsx')\n",
        "\n",
        "# Extract the closing prices as the target variable\n",
        "target = data['Turnover (Lacs)'].values\n",
        "\n",
        "# Normalize the target variable\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target = scaler.fit_transform(target.reshape(-1, 1))\n",
        "\n",
        "# Prepare the input data\n",
        "features = ['Open', 'High', 'Low', 'Last','Close','Total Trade Quantity']\n",
        "inputs = data[features].values\n",
        "\n",
        "# Normalize the input data\n",
        "inputs = scaler.fit_transform(inputs)\n",
        "\n",
        "# Define the input shape for the LSTM model\n",
        "timesteps = 10  # Number of time steps to consider\n",
        "n_features = len(features)  # Number of input features\n",
        "\n",
        "# Reshape the input data into a 3D array\n",
        "X = []\n",
        "y = []\n",
        "for i in range(timesteps, len(inputs)):\n",
        "    X.append(inputs[i - timesteps:i])\n",
        "    y.append(target[i])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the stacked LSTM model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(units=64, return_sequences=True, input_shape=(timesteps, n_features)))\n",
        "model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
        "model.add(keras.layers.LSTM(units=64, return_sequences=False))\n",
        "model.add(keras.layers.Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsSkye6jHQ4N",
        "outputId": "35910f8c-3138-4a58-f5ca-7d4d02570710"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 7s 19ms/step - loss: 0.0049\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0042\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0041\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0041\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0040\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0043\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0040\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0040\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 2s 35ms/step - loss: 0.0041\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 1s 19ms/step - loss: 0.0039\n",
            "13/13 [==============================] - 1s 6ms/step - loss: 0.0032\n",
            "Loss: 0.0031536296010017395\n",
            "13/13 [==============================] - 1s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predictions:\\n\")\n",
        "print(np.transpose((predictions*100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cqC-3aII-Bz",
        "outputId": "baee31da-529e-4303-c145-5723fb9b87e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "\n",
            "[[ 4.9904385   0.5927811   5.760497    9.236221    5.7371564   1.0697742\n",
            "  17.968185    6.9247527   6.1687083  13.386323    6.1338468   1.6365267\n",
            "   0.60826534  7.315061    5.729255    3.3251505   8.105381    6.3801155\n",
            "   5.1628857  14.849408    6.4990377   7.7570915  17.830431   17.154154\n",
            "   0.866588    6.129691    9.430291    6.5029426   3.8082533   3.531062\n",
            "   4.613272    5.024746    4.23984     3.901328   16.495333    4.4602513\n",
            "   5.656933    6.6679344   0.35227403  4.5971794  18.246593    5.897445\n",
            "   8.8923      3.8648703   3.490875    5.0971723   2.2137527   2.9564776\n",
            "   5.334626    5.964875    2.6162715   9.18997    24.897964    5.8734903\n",
            "  13.341366    8.841618    3.336031    1.3276597   2.262943    7.9855523\n",
            "   7.347307    7.6696715   6.527665   24.307375   17.214674   18.802715\n",
            "   3.3460193   2.2560983   7.5466814   0.3407826   0.95737976  2.5319421\n",
            "  13.533958    8.812684    5.0803795   7.1846194   6.464956    4.6147714\n",
            "   6.0881114   9.471653   13.012145    8.320749    5.736045    7.063684\n",
            "   6.5541797   0.9192532   2.1110463   4.9087434   5.963781   17.166807\n",
            "   2.9523032   4.7491975   5.9556627   4.7899203  20.40624     1.9454303\n",
            "   3.7517474   5.232452    4.0768294   7.1500077   4.1861854   6.500134\n",
            "   5.8494596   7.095647    6.621497    4.4227467   5.8945103   5.7971478\n",
            "   4.783742    4.009889    1.3632524   6.921972    7.022358    4.943865\n",
            "   5.283765    3.9557145  12.229952    3.5839617   5.2618914   9.290913\n",
            "   4.141099    0.25159413  3.330812    6.80156    10.626364    7.0684686\n",
            "  16.446402    1.281683    4.022233    6.4467406   3.5614483   8.102463\n",
            "   3.5797687   5.7016454   4.006727    0.85523164  0.954759    8.127275\n",
            "   7.5100765   5.472499    7.5924454   5.3593807   7.887864    6.1891003\n",
            "   7.832314    8.107858    3.1332746   4.0505395   0.8555458   3.1672678\n",
            "  18.949308    2.8791604   1.6571324   3.5352468   2.2580807   3.1022046\n",
            "  14.001346    7.8641357   5.3991804   8.078353    6.244575    5.3965936\n",
            "  17.76759     5.4289727   9.77718     7.253459    2.1758208  22.938717\n",
            "  19.547163    6.4500494   5.5012302   2.5789297   6.4214864   5.823344\n",
            "   3.8095632   4.0947533   3.1826482  18.917233    2.385838    2.9802716\n",
            "  12.63995     7.5264463   9.497462    6.418665    4.1473413   4.639424\n",
            "   7.8808546   6.0824513   5.331292    5.5396585   5.906101    3.3837094\n",
            "   5.5281982   6.757405    9.297779   14.69878     0.67499065  2.8849232\n",
            "   4.741595    7.2445498   3.2317505   9.360155    2.301432    2.1972742\n",
            "   4.224498    2.3170161   1.6790916   8.094146    4.1967196  12.410516\n",
            "   2.864647   17.256603    6.161814    4.6787868  22.704807    8.661879\n",
            "   3.9473765  11.081764    5.152745    5.1290197  22.525301    3.26427\n",
            "   4.637498    7.0348577  14.156273    5.361704    2.926955    3.7771642\n",
            "  14.350757    1.6410427  18.27474     6.6055827  23.355085    6.188713\n",
            "   6.161747    0.83962864  7.1554527   8.177755    2.1251693   4.953917\n",
            "  23.123486    3.086387    3.3987894   7.9927416   9.1386795   5.90038\n",
            "  20.237144    0.32845074  5.7578735   3.853056   18.969934    5.162745\n",
            "   1.6270981   8.332816    9.650549    5.620768    2.976436    3.7007518\n",
            "   5.0157666   6.615057    3.3613167   4.1199794   3.2540135   2.2061896\n",
            "   5.243415    6.0090737   6.5992513   7.7133923   4.6734543   7.040526\n",
            "  17.210943    6.4916744  17.495188    2.954709    6.7952185   6.383309\n",
            "   3.0328283  22.105722   22.47445     4.4281116   3.3366137   4.9616756\n",
            "   5.987824    5.4678745   3.9452121   7.3091984   7.321523    7.411465\n",
            "   3.8228831  17.15341     3.9014406   6.8566227   5.5691266   6.498048\n",
            "   7.5773044  17.55586     5.109951    5.6493053  17.214949   21.8935\n",
            "   3.367126    5.9791408   6.9646163  15.376706    3.3158138   5.4061832\n",
            "  16.215721    3.7294886   6.003084    6.010054    5.098952   17.543837\n",
            "   3.292019    9.232877    1.1570017   3.8628585   0.809273    3.972572\n",
            "   0.38429257 17.6748      2.8461094   3.2517574   7.1783724   7.4190283\n",
            "   5.139474    4.937944    1.901903    2.631783    3.2432458   6.094632\n",
            "   4.271482   16.24366     5.4820776  25.498608    9.289635    0.2874229\n",
            "   3.0090327   3.7701607   6.868735    9.101754    5.3050766   5.06582\n",
            "   4.9315014   2.8377821   6.595432    5.7696095   0.69926894  6.567858\n",
            "   8.23942    23.403091    1.8794993   6.3200703   9.710468    9.530854\n",
            "  11.848205    5.246381    2.1710186   6.251437    3.7312396   6.2507257\n",
            "   4.012973    3.8085341   2.2901428  21.707325    9.379733    3.4514844\n",
            "   5.645345   17.03724     6.935069    5.065747    7.773517   15.562434\n",
            "   3.061201    4.353881    5.856734    8.187291   22.120201    6.026888\n",
            "   6.349374    0.9694262   7.230328    8.428486   16.018713    5.8766866\n",
            "  10.409201   17.561363    7.1170607   3.37537     5.3656673   4.58227\n",
            "   5.9918985   6.253428    9.031591    6.292261    4.7525682   2.4101431\n",
            "   3.0393178   2.3526561   3.501559    2.8592887   0.29587048  3.169301\n",
            "   8.387163    2.3349056   1.470189  ]]\n"
          ]
        }
      ]
    }
  ]
}